# pack.py

import struct
import torch
import struct

def flatten_list(sublists):
    return [item for sublist in sublists for item in sublist]

def int_to_bytes_list(n):
    binary_str = bin(n)[2:].zfill(32)
    return [int(binary_str[i:i + 8], 2) for i in range(0, 32, 8)]

def float16_to_bytes(value):
    float16_value = torch.tensor(value, dtype=torch.float16)
    packed_bytes = struct.pack('>e', float16_value.item())
    return list(packed_bytes) + [0] * (4 - len(packed_bytes))

def bfloat16_to_bytes(number):
    number_unpacked = struct.unpack('!I', struct.pack('!f', number))[0]
    res_masked = number_unpacked & 0xFFFF0000
    return int_to_bytes_list(res_masked)

def fp32_to_bytes(number):
    number_unpacked = struct.unpack('!I', struct.pack('!f', number))[0]
    return int_to_bytes_list(number_unpacked)    

def int32_to_bytes(number):
    number_unpacked = struct.unpack('!I', struct.pack('!I', number))[0]
    return int_to_bytes_list(number_unpacked)    

def bfloat16_to_binary(value):
    float_value = value.to(torch.float32).item()
    bfloat16_bytes = bfloat16_to_bytes(float_value)
    return f"{bfloat16_bytes[0]:08b}{bfloat16_bytes[1]:08b}"

def pack_bfp16(torch_tensor):
    packed_bytes = []
    for i in range(0, len(torch_tensor), 2):
        half1 = bfloat16_to_bytes(torch_tensor[i])
        half2 = bfloat16_to_bytes(torch_tensor[i + 1])
        packed_bytes.extend([half1[0:2][::-1], half2[0:2][::-1]][::-1]) 
    return flatten_list(packed_bytes)

def pack_fp16(torch_tensor):
    packed_bytes = []
    for i in range(0, len(torch_tensor), 2):
        half1 = float16_to_bytes(torch_tensor[i])
        half2 = float16_to_bytes(torch_tensor[i + 1])
        packed_bytes.extend([half1[0:2][::-1], half2[0:2][::-1]][::-1]) 
    return flatten_list(packed_bytes)

def pack_fp32(torch_tensor):
    packed_bytes = []
    for i in range(0, len(torch_tensor)):
        packed_bytes.append(fp32_to_bytes(torch_tensor[i])[::-1])
    return flatten_list(packed_bytes)   

def pack_int32(torch_tensor):
    packed_bytes = []
    for i in range(0, len(torch_tensor)):
        packed_bytes.append(int32_to_bytes(torch_tensor[i])[::-1])
    return flatten_list(packed_bytes)   

def float_to_bfp8_block(block):
    exponents = []
    mantissas = []
    signs = []
    max_exponent = -float('inf')
    
    for value in block:
        binary_str = bfloat16_to_binary(value)
        sign = binary_str[0]
        signs.append(int(sign,2))
        exponent = int(binary_str[1:9], 2)
        mantissa = binary_str[9:-1] # remove last
        mantissa = "1" + mantissa ## add 1
        exponents.append(exponent)
        mantissas.append(mantissa)
        max_exponent = max(max_exponent, exponent)
    
    shared_exponent = max_exponent

    mantissas_explicit = [int(mantissa,2) for mantissa in mantissas]

    bfp8_mantissas = []
    for i in range(len(block)):
        exponent_delta = shared_exponent - exponents[i]
        mantissa = mantissas_explicit[i] >> exponent_delta
        mantissa = (signs[i] << 7) | mantissa
        bfp8_mantissas.append(mantissa)

    return shared_exponent, bfp8_mantissas

def pack_bfp8_b(tensor, block_size=16):
    flattened_tensor = tensor.flatten()
    num_blocks = len(flattened_tensor) // block_size 
    blocks = [flattened_tensor[i * block_size:(i + 1) * block_size] for i in range(num_blocks)]
    
    exponents = []
    mantissas = []
    
    for block in blocks:
        shared_exponent, bfp8_mantissas = float_to_bfp8_block(block)
        exponents.append(shared_exponent)
        mantissas.extend(bfp8_mantissas)
    
    bfp8_result = exponents + mantissas
    
    return bfp8_result